{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Research Paper Engine using arXiv, LangChain ü¶úÔ∏èüîó and Google Gemini"
      ],
      "metadata": {
        "id": "PetNuzxJZwow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Tahreem Rasul](https://github.com/tahreemrasul) |"
      ],
      "metadata": {
        "id": "xvIyW-EMukRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/tahreemrasul/rag_research_paper_engine_workshop/blob/main/rag_research_paper_engine.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/tahreemrasul/rag_research_paper_engine_workshop/blob/main/rag_research_paper_engine.ipynb\">\n",
        "      <img width=\"28px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "w82RyFPUG4W-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates implementing a research paper engine using the arXiv API to show how to improve LLM's response by augmenting LLM's knowledge with external data sources such as documents. The notebooks uses Vertex AI Gemini Pro 1.0 for Text, Embeddings for Text API, arXiv API and LangChain ü¶úÔ∏èüîó.\n",
        "\n",
        "## Context\n",
        "\n",
        "Large Language Models (LLMs) have improved quantitatively and qualitatively. They can learn new abilities without being directly trained on them. However, there are constraints with LLMs - they are unaware of events after training and it is almost impossible to trace the sources to their responses. It is preferred for LLM based systems to cite their sources and be grounded in facts.\n",
        "\n",
        "To solve for the constraints, one of the approaches is to augment the prompt sent to LLM with relevant data retrieved from an external knowledge base through Information Retrieval (IR) mechanism.\n",
        "\n",
        "This approach is called Retrieval Augmented Generation (RAG), also known as Generative QA in the context of a Question Answering task. There are two main components in RAG based architecture: (1) Retriever and (2) Generator."
      ],
      "metadata": {
        "id": "S-KL6H18sXA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "### Install Vertex AI SDK, other packages and their dependencies\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ],
      "metadata": {
        "id": "9dVwHu5gs-dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LangChain and related packages\n",
        "!pip install --upgrade --quiet langchain langchain-google-vertexai langchain-community chromadb arxiv pymupdf"
      ],
      "metadata": {
        "id": "JqZuoY_dZ7G8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ],
      "metadata": {
        "id": "SKByjcaPtDHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgtqfYu9jf7o",
        "outputId": "8af7f878-ef17-4fef-d3dc-a60d8ac3034b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>‚ö†Ô∏è Before proceeding, please wait for the kernel to finish restarting ‚ö†Ô∏è</b>\n",
        "</div>"
      ],
      "metadata": {
        "id": "v2cbF6nptJu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticating your notebook environment\n",
        "\n",
        "If you are using Colab, you will need to authenticate yourself first. The next cell will check if you are currently using Colab, and will start the authentication process.\n",
        "\n",
        "If you are using Vertex AI Workbench, you will not require additional authentication.\n",
        "\n",
        "For more information, you can check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
      ],
      "metadata": {
        "id": "V2-PTrJjuHVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "vPo0ch83ounD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Relevant Papers from arXiv API\n",
        "\n",
        "This step retrieves relevant research papers based on the user query. The document corpus used as dataset will be the research papers pulled from the `arXiv` API. We will be using the `ArxivLoader` class from LangChain to load the PDFs of these papers."
      ],
      "metadata": {
        "id": "wFJUl7RhpYdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query & No. of Papers { display-mode: \"form\" }\n",
        "query = \"neural networks\"  # @param {type:\"string\"}\n",
        "\n",
        "# @title Total Docs { display-mode: \"form\" }\n",
        "num_papers = \"3\"  # @param {type: \"string\"}"
      ],
      "metadata": {
        "id": "JsTU3RZkx7dd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import ArxivLoader\n",
        "\n",
        "arxiv_docs = ArxivLoader(query=query, load_max_docs=int(num_papers)).load()"
      ],
      "metadata": {
        "id": "tOBHwVySxepN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once retreived, display the metadata to check which papers were returned"
      ],
      "metadata": {
        "id": "2661AqkeqwlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(int(num_papers)):\n",
        "  print(f\"Paper # {i+1}:\")\n",
        "  print(f\"Published: {arxiv_docs[i].metadata['Published']}\")\n",
        "  print(f\"Title: {arxiv_docs[i].metadata['Title']}\")\n",
        "  print(f\"Authors: {arxiv_docs[i].metadata['Authors']}\")\n",
        "  print(f\"Summary: {arxiv_docs[i].metadata['Summary']}\")\n",
        "  print('------------------------------------------------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPOwT8JRy3Ww",
        "outputId": "343f0880-09e6-4442-9e46-a3111f881aa6",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper # 1:\n",
            "Published: 2023-04-18\n",
            "Title: Lecture Notes: Neural Network Architectures\n",
            "Authors: Evelyn Herberg\n",
            "Summary: These lecture notes provide an overview of Neural Network architectures from\n",
            "a mathematical point of view. Especially, Machine Learning with Neural Networks\n",
            "is seen as an optimization problem. Covered are an introduction to Neural\n",
            "Networks and the following architectures: Feedforward Neural Network,\n",
            "Convolutional Neural Network, ResNet, and Recurrent Neural Network.\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "Paper # 2:\n",
            "Published: 2023-11-18\n",
            "Title: Bayesian Neural Networks: A Min-Max Game Framework\n",
            "Authors: Junping Hong, Ercan Engin Kuruoglu\n",
            "Summary: Bayesian neural networks use random variables to describe the neural networks\n",
            "rather than deterministic neural networks and are mostly trained by variational\n",
            "inference which updates the mean and variance at the same time. Here, we\n",
            "formulate the Bayesian neural networks as a minimax game problem. We do the\n",
            "experiments on the MNIST data set and the primary result is comparable to the\n",
            "existing closed-loop transcription neural network. Finally, we reveal the\n",
            "connections between Bayesian neural networks and closed-loop transcription\n",
            "neural networks, and show our framework is rather practical, and provide\n",
            "another view of Bayesian neural networks.\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "Paper # 3:\n",
            "Published: 2005-04-13\n",
            "Title: Self-Organizing Multilayered Neural Networks of Optimal Complexity\n",
            "Authors: V. Schetinin\n",
            "Summary: The principles of self-organizing the neural networks of optimal complexity\n",
            "is considered under the unrepresentative learning set. The method of\n",
            "self-organizing the multi-layered neural networks is offered and used to train\n",
            "the logical neural networks which were applied to the medical diagnostics.\n",
            "------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunk documents - TextSplitter\n",
        "\n",
        "Split the documents retrieved into smaller chunks. When splitting the document, ensure a few chunks can fit within the context length of LLM."
      ],
      "metadata": {
        "id": "mCBqPg7hq0ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "pdf_data = []\n",
        "for doc in arxiv_docs:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    doc_splits = text_splitter.create_documents([doc.page_content])\n",
        "    for idx, split in enumerate(doc_splits):\n",
        "      split.metadata[\"chunk\"] = idx\n",
        "    pdf_data.append(doc_splits)\n",
        "\n",
        "print(f\"# of pdfs = {len(pdf_data)} \\n# of split documents = {sum([len(doc_splits) for doc_splits in pdf_data])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5IDeM1g_lpc",
        "outputId": "0d7ba42e-6f5c-4ed7-c32f-06aafdc9c076"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of pdfs = 3 \n",
            "# of split documents = 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the VertexAI Embedding model"
      ],
      "metadata": {
        "id": "w21N8p80rqKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Project { display-mode: \"form\" }\n",
        "PROJECT_ID = \"build-with-ai-424207\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "\n",
        "# @title Region { display-mode: \"form\" }\n",
        "REGION = \"US-central-1\"  # @param {type: \"string\"}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf8_Q7mApLmM",
        "outputId": "b621d187-a7bd-41c9-c2b2-10ff490240c7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "embedding_model = VertexAIEmbeddings(\n",
        "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
        ")\n",
        "print(embedding_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68qVOQKyuQoT",
        "outputId": "5d0c20cf-4926-45b6-9e44-435751d449e7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=<vertexai.language_models.TextEmbeddingModel object at 0x7f37607ed5d0> async_client=None project='build-with-ai-424207' location='us-central1' request_parallelism=5 max_retries=6 stop=None model_name='textembedding-gecko@latest' model_family=None full_model_name=None client_preview=None temperature=None max_output_tokens=None top_p=None top_k=None credentials=None n=1 streaming=False safety_settings=None api_transport=None api_endpoint=None tuned_model_name=None instance={'max_batch_size': 250, 'batch_size': 250, 'min_batch_size': 5, 'min_good_batch_size': 5, 'lock': <unlocked _thread.lock object at 0x7f376213b540>, 'batch_size_validated': False, 'task_executor': <concurrent.futures.thread.ThreadPoolExecutor object at 0x7f376146fb50>, 'embeddings_task_type_supported': True, 'get_embeddings_with_retry': <function _TextEmbeddingModel.get_embeddings at 0x7f3769550310>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `ChromaDB` as Vector Store\n",
        "\n",
        "This step generates embeddings from the documents and adds the embeddings to the vector store. The vector store being used is the `Chroma` database.\n"
      ],
      "metadata": {
        "id": "I_5nT5YVozTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud services enable aiplatform.googleapis.com\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "db = Chroma.from_documents(pdf_data[0], embedding_model)"
      ],
      "metadata": {
        "id": "lqyY4z2STxNz",
        "outputId": "46b93e57-cabc-4a7d-aea0-184db92c2fe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying vertexai.language_models._language_models._TextEmbeddingModel.get_embeddings in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying vertexai.language_models._language_models._TextEmbeddingModel.get_embeddings in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying vertexai.language_models._language_models._TextEmbeddingModel.get_embeddings in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying vertexai.language_models._language_models._TextEmbeddingModel.get_embeddings in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying vertexai.language_models._language_models._TextEmbeddingModel.get_embeddings in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: textembedding-gecko. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title search query { display-mode: \"form\" }\n",
        "search_query = \"What should be considered when taking derivatives of ReLU?\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "skgUgy7kga3N"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify the `ChromaDB` with similarity search"
      ],
      "metadata": {
        "id": "r_fuacRXpPV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db.similarity_search(\n",
        "    search_query\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S48Gk44We3rI",
        "outputId": "54233078-2381-4f0a-a496-1f89d4419fe2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='identity function often helps speed up convergence, since it resembles a linear model, as long as\\nthe values are close to zero. Another challenge that needs to be overcome is vanishing derivatives,\\nwhich is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not\\nbounded on positive values, while also being comparatively cheap to compute, because linear\\ncomputations tend to be very well optimized in modern computing. Altogether, these advan-\\ntages have resulted in ReLU (and variants thereof) becoming the most widely used activation\\nfunction currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was\\nintroduced. When taking derivatives of ReLU one needs to account for the non-diÔ¨Äerentiability\\nat 0, but in numerical practice this is easily overcome.\\nWith the help of Neural Networks we want to solve a task, cf.\\n[15, Section 5.1].\\nLet the\\nperformance of the algorithm for the given task be measured by the loss function L, which', metadata={'chunk': 8}),\n",
              " Document(page_content='identity function often helps speed up convergence, since it resembles a linear model, as long as\\nthe values are close to zero. Another challenge that needs to be overcome is vanishing derivatives,\\nwhich is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not\\nbounded on positive values, while also being comparatively cheap to compute, because linear\\ncomputations tend to be very well optimized in modern computing. Altogether, these advan-\\ntages have resulted in ReLU (and variants thereof) becoming the most widely used activation\\nfunction currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was\\nintroduced. When taking derivatives of ReLU one needs to account for the non-diÔ¨Äerentiability\\nat 0, but in numerical practice this is easily overcome.\\nWith the help of Neural Networks we want to solve a task, cf.\\n[15, Section 5.1].\\nLet the\\nperformance of the algorithm for the given task be measured by the loss function L, which', metadata={'chunk': 8}),\n",
              " Document(page_content='‚áí\\ny[‚Ñì] ‚àíy[‚Ñì‚àí1]\\nœÑ [‚Ñì]\\n= œÉ(W [‚Ñì‚àí1]y[‚Ñì‚àí1] + b[‚Ñì‚àí1]).\\n4\\nRESNET\\n38\\nHere, we consider the same activation function œÉ for all layers. Now, the left hand side of the\\nequation can be interpreted as a Ô¨Ånite diÔ¨Äerence representation of a time derivative, where œÑ [‚Ñì] is\\nthe time step size and y[‚Ñì], y[‚Ñì‚àí1] are the values attained at two neighboring points in time. This\\nrelation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is\\nalso possible to learn the time step size œÑ [‚Ñì] as an additional variable, [2].\\nLet us now introduce the diÔ¨Äerent ResNet versions from the original papers, [17, 19].\\n4.1.\\nDiÔ¨Äerent ResNet Versions\\nIn contrast to the simpliÔ¨Åed ResNet layer version (9) that we introduced, original ResNet ar-\\nchitectures [17] consist of residual blocks, cf. Figure 28. Here, diÔ¨Äerent layers are grouped\\ntogether into one residual block and then residual blocks are stacked to form a ResNet.\\ny[‚Ñì‚àí1]\\nWeights\\nBN\\nReLU\\nWeights\\nBN\\n+\\nReLU\\ny[‚Ñì]', metadata={'chunk': 83}),\n",
              " Document(page_content='‚áí\\ny[‚Ñì] ‚àíy[‚Ñì‚àí1]\\nœÑ [‚Ñì]\\n= œÉ(W [‚Ñì‚àí1]y[‚Ñì‚àí1] + b[‚Ñì‚àí1]).\\n4\\nRESNET\\n38\\nHere, we consider the same activation function œÉ for all layers. Now, the left hand side of the\\nequation can be interpreted as a Ô¨Ånite diÔ¨Äerence representation of a time derivative, where œÑ [‚Ñì] is\\nthe time step size and y[‚Ñì], y[‚Ñì‚àí1] are the values attained at two neighboring points in time. This\\nrelation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is\\nalso possible to learn the time step size œÑ [‚Ñì] as an additional variable, [2].\\nLet us now introduce the diÔ¨Äerent ResNet versions from the original papers, [17, 19].\\n4.1.\\nDiÔ¨Äerent ResNet Versions\\nIn contrast to the simpliÔ¨Åed ResNet layer version (9) that we introduced, original ResNet ar-\\nchitectures [17] consist of residual blocks, cf. Figure 28. Here, diÔ¨Äerent layers are grouped\\ntogether into one residual block and then residual blocks are stacked to form a ResNet.\\ny[‚Ñì‚àí1]\\nWeights\\nBN\\nReLU\\nWeights\\nBN\\n+\\nReLU\\ny[‚Ñì]', metadata={'chunk': 83})]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval based Question/Answering Chain\n",
        "\n",
        "We will demonstrate using three LangChain retrieval Q&A chains:\n",
        "\n",
        "- `RetrievalQA`\n",
        "- `ConversationalRetrievalChain`\n",
        "- Advanced: customized Q&A prompt and format\n",
        "\n",
        "We begin by initializing a Vertex AI LLM and a LangChain retriever to fetch documents from our Chroma Database containing ingested pdfs of papers we fetched earlier.\n",
        "\n",
        "For Q&A chains our retriever is passed directly to the chain and can be used automatically without any further configuration.\n",
        "\n",
        "Behind the scenes, first the search query is passed to the retriever which runs a search and returns relevant document chunks.\n",
        "\n",
        "These chunks are then passed to the prompt used by the LLM to be used as context."
      ],
      "metadata": {
        "id": "C9NEgcimmk2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = VertexAI(model_name=\"gemini-pro\")\n",
        "\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "IET1BDbmfWRd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `RetrievalQA` chain\n",
        "\n",
        "This is the simplest document Q&A chain offered by LangChain.\n",
        "\n",
        "There are several different chain types available.\n",
        "\n",
        "- In these examples we use the `stuff` type, which simply inserts all of the document chunks into the prompt.\n",
        "- This has the advantage of only making a single LLM call, which is faster and more cost efficient.\n",
        "- However, if we have a large number of search results we run the risk of exceeding the token limit in our prompt, or truncating useful information.\n",
        "- Other chain types such as `map_reduce` and `refine` use an iterative process which makes multiple LLM calls, taking individual document chunks at a time and refining the answer iteratively."
      ],
      "metadata": {
        "id": "8F8qhv5dnSbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                           chain_type=\"stuff\",\n",
        "                                           retriever=retriever)\n",
        "\n",
        "retrieval_qa.invoke(search_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAugZ8D8oHPK",
        "outputId": "a7e09502-7d05-4366-83dd-ebb809f9d051"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What should be considered when taking derivatives of ReLU?',\n",
              " 'result': 'Here is what should be considered when taking derivatives of ReLU: \\n\\n* ReLU is not differentiable at 0. This means that the derivative of ReLU is undefined at 0. In practice, this is usually handled by using a small epsilon value to approximate the derivative at 0. For example, the derivative of ReLU at 0 could be approximated as $\\\\epsilon$ or $-\\\\epsilon$, where $\\\\epsilon$ is a small positive number.\\n* ReLU may suffer from the \"dying ReLU\" problem. This problem occurs when a ReLU neuron gets stuck in a state where it is always outputting 0. This can happen if the input to the neuron is always negative, or if the weights of the neuron are very small. When a ReLU neuron is stuck in the \"dying\" state, it can no longer learn anything new.\\n* ReLU is not as robust to noise as other activation functions, such as the sigmoid function or the tanh function. This is because ReLU is a piecewise linear function, which means that small changes in the input can lead to large changes in the output.\\n* ReLU can be computationally expensive to compute, especially on large datasets. This is because ReLU requires the computation of a maximum function, which can be slow.\\n\\nOverall, ReLU is a popular activation function that has been shown to be very effective in many different tasks. However, it is important to be aware of the potential pitfalls of ReLU before using it in your own models.'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inspecting the process\n",
        "\n",
        "If we add `return_source_documents=True` we can inspect the document chunks that were returned by the retriever.\n",
        "\n",
        "This is helpful for debugging, as these chunks may not always be relevant to the answer, or their relevance might not be obvious."
      ],
      "metadata": {
        "id": "dVYKSS3QoHlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=retriever,\n",
        "                                 return_source_documents=True)\n",
        "\n",
        "results = retrieval_qa.invoke(search_query)\n",
        "\n",
        "print(\"*\" * 79)\n",
        "print(results[\"result\"])\n",
        "print(\"*\" * 79)\n",
        "for doc in results[\"source_documents\"]:\n",
        "    print(\"-\" * 79)\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H-UoBsfhfjr8",
        "outputId": "d125f892-7c67-4dee-822b-a6b8d1eb3803"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************************************\n",
            "The text provided doesn't specifically discuss taking derivatives of ReLU, but it does mention that ReLU is not differentiable at 0. When taking derivatives of ReLU, you need to account for this non-differentiability at 0. However, the text mentions that in numerical practice this is easily overcome. So, while being aware of the non-differentiability at 0 is important, it doesn't seem to be a significant challenge when working with ReLU in practice. \n",
            "\n",
            "*******************************************************************************\n",
            "-------------------------------------------------------------------------------\n",
            "identity function often helps speed up convergence, since it resembles a linear model, as long as\n",
            "the values are close to zero. Another challenge that needs to be overcome is vanishing derivatives,\n",
            "which is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not\n",
            "bounded on positive values, while also being comparatively cheap to compute, because linear\n",
            "computations tend to be very well optimized in modern computing. Altogether, these advan-\n",
            "tages have resulted in ReLU (and variants thereof) becoming the most widely used activation\n",
            "function currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was\n",
            "introduced. When taking derivatives of ReLU one needs to account for the non-diÔ¨Äerentiability\n",
            "at 0, but in numerical practice this is easily overcome.\n",
            "With the help of Neural Networks we want to solve a task, cf.\n",
            "[15, Section 5.1].\n",
            "Let the\n",
            "performance of the algorithm for the given task be measured by the loss function L, which\n",
            "-------------------------------------------------------------------------------\n",
            "identity function often helps speed up convergence, since it resembles a linear model, as long as\n",
            "the values are close to zero. Another challenge that needs to be overcome is vanishing derivatives,\n",
            "which is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not\n",
            "bounded on positive values, while also being comparatively cheap to compute, because linear\n",
            "computations tend to be very well optimized in modern computing. Altogether, these advan-\n",
            "tages have resulted in ReLU (and variants thereof) becoming the most widely used activation\n",
            "function currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was\n",
            "introduced. When taking derivatives of ReLU one needs to account for the non-diÔ¨Äerentiability\n",
            "at 0, but in numerical practice this is easily overcome.\n",
            "With the help of Neural Networks we want to solve a task, cf.\n",
            "[15, Section 5.1].\n",
            "Let the\n",
            "performance of the algorithm for the given task be measured by the loss function L, which\n",
            "-------------------------------------------------------------------------------\n",
            "‚áí\n",
            "y[‚Ñì] ‚àíy[‚Ñì‚àí1]\n",
            "œÑ [‚Ñì]\n",
            "= œÉ(W [‚Ñì‚àí1]y[‚Ñì‚àí1] + b[‚Ñì‚àí1]).\n",
            "4\n",
            "RESNET\n",
            "38\n",
            "Here, we consider the same activation function œÉ for all layers. Now, the left hand side of the\n",
            "equation can be interpreted as a Ô¨Ånite diÔ¨Äerence representation of a time derivative, where œÑ [‚Ñì] is\n",
            "the time step size and y[‚Ñì], y[‚Ñì‚àí1] are the values attained at two neighboring points in time. This\n",
            "relation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is\n",
            "also possible to learn the time step size œÑ [‚Ñì] as an additional variable, [2].\n",
            "Let us now introduce the diÔ¨Äerent ResNet versions from the original papers, [17, 19].\n",
            "4.1.\n",
            "DiÔ¨Äerent ResNet Versions\n",
            "In contrast to the simpliÔ¨Åed ResNet layer version (9) that we introduced, original ResNet ar-\n",
            "chitectures [17] consist of residual blocks, cf. Figure 28. Here, diÔ¨Äerent layers are grouped\n",
            "together into one residual block and then residual blocks are stacked to form a ResNet.\n",
            "y[‚Ñì‚àí1]\n",
            "Weights\n",
            "BN\n",
            "ReLU\n",
            "Weights\n",
            "BN\n",
            "+\n",
            "ReLU\n",
            "y[‚Ñì]\n",
            "-------------------------------------------------------------------------------\n",
            "‚áí\n",
            "y[‚Ñì] ‚àíy[‚Ñì‚àí1]\n",
            "œÑ [‚Ñì]\n",
            "= œÉ(W [‚Ñì‚àí1]y[‚Ñì‚àí1] + b[‚Ñì‚àí1]).\n",
            "4\n",
            "RESNET\n",
            "38\n",
            "Here, we consider the same activation function œÉ for all layers. Now, the left hand side of the\n",
            "equation can be interpreted as a Ô¨Ånite diÔ¨Äerence representation of a time derivative, where œÑ [‚Ñì] is\n",
            "the time step size and y[‚Ñì], y[‚Ñì‚àí1] are the values attained at two neighboring points in time. This\n",
            "relation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is\n",
            "also possible to learn the time step size œÑ [‚Ñì] as an additional variable, [2].\n",
            "Let us now introduce the diÔ¨Äerent ResNet versions from the original papers, [17, 19].\n",
            "4.1.\n",
            "DiÔ¨Äerent ResNet Versions\n",
            "In contrast to the simpliÔ¨Åed ResNet layer version (9) that we introduced, original ResNet ar-\n",
            "chitectures [17] consist of residual blocks, cf. Figure 28. Here, diÔ¨Äerent layers are grouped\n",
            "together into one residual block and then residual blocks are stacked to form a ResNet.\n",
            "y[‚Ñì‚àí1]\n",
            "Weights\n",
            "BN\n",
            "ReLU\n",
            "Weights\n",
            "BN\n",
            "+\n",
            "ReLU\n",
            "y[‚Ñì]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationalRetrievalChain\n",
        "`ConversationalRetrievalChain` remembers and uses previous questions so you can have a chat-like discovery process.\n",
        "\n",
        "To use this chain we must provide a memory class to store and pass the previous messages to the LLM as context. Here we use the `ConversationBufferMemory` class that comes with LangChain."
      ],
      "metadata": {
        "id": "bqNFe4A8hBhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversational_retrieval = ConversationalRetrievalChain.from_llm(llm=llm,\n",
        "                                                                 retriever=retriever,\n",
        "                                                                 memory=memory)\n",
        "\n",
        "\n",
        "conversational_retrieval.invoke(search_query)[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "VTzwlThrCcrF",
        "outputId": "e8d7d91a-a9da-4d0b-9612-03bd37295429"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The answer to your question can be found in the text you provided. When taking derivatives of ReLU one needs to account for the non-diÔ¨Äerentiability at 0. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_query = \"What about other activation functions?\"\n",
        "result = conversational_retrieval.invoke(new_query)\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVqxz1Wah9X3",
        "outputId": "d22da5c7-9014-4592-aa70-a45ad7e0bdcf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Considerations when taking derivatives of other activation functions:\n",
            "\n",
            "While your provided context mentions the challenges of taking derivatives for Heaviside, sigmoid, and hyperbolic tangent functions, let's focus on **ReLU** and **Leaky ReLU** as the question prompts.\n",
            "\n",
            "### ReLU (Rectified Linear Unit)\n",
            "\n",
            "* **Non-differentiability at 0:** The biggest challenge with ReLU is its non-differentiability at 0. This means that the derivative is undefined at that point, causing issues for gradient-based optimization algorithms.\n",
            "* **Practical solutions:** In practice, this is often handled by considering the derivative as 0 at 0. Alternatively, a small positive value can be used instead, resulting in a \"smoothed\" version of the derivative at 0.\n",
            "* **Computational efficiency:** ReLU is computationally efficient as it only involves a simple comparison and thresholding operation, making it attractive for large-scale deep learning tasks.\n",
            "\n",
            "### Leaky ReLU\n",
            "\n",
            "* **Addressing vanishing gradients:** Leaky ReLU was introduced to address the vanishing gradient problem that can occur with ReLU for negative values. It introduces a small non-zero slope for negative inputs, allowing gradients to flow through even when the activation is not positive.\n",
            "* **Parameter Œ±:** The parameter Œ± controls the slope of the negative part of the function. A larger Œ± leads to a less \"leaky\" function, approaching the behavior of ReLU.\n",
            "* **Differentiability:** Leaky ReLU is differentiable everywhere, making it more amenable to gradient-based optimization compared to ReLU.\n",
            "\n",
            "### Summary\n",
            "\n",
            "Taking derivatives of ReLU and Leaky ReLU requires specific considerations due to their non-differentiability at 0 (ReLU) and the presence of a parameter Œ± (Leaky ReLU). Practical solutions and computational efficiency are important factors to consider depending on the application and computational resources available.\n",
            "\n",
            "If you need further information or clarification on any specific aspect of these activation functions, please ask!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_query = \"give me specifically for sigmoid\"\n",
        "result = conversational_retrieval.invoke(new_query)\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exXwDy-9iS4w",
        "outputId": "2c4c1b5f-16d9-4b6d-8e0c-22cf61c5e9a9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The derivative of the sigmoid function is $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$, where $\\sigma(x)$ is the sigmoid function, defined as $\\sigma(x) = \\frac{1}{1 + e^{-x}}$. \n",
            "\n",
            "This formula can be derived using the chain rule and the definition of the sigmoid function. For example,\n",
            "```\n",
            "\\begin{aligned}\n",
            "\\frac{d}{dx} \\sigma(x) &= \\frac{d}{dx} \\left( \\frac{1}{1 + e^{-x}} \\right) \\\\\n",
            "&= \\frac{(1 + e^{-x})(-e^{-x})}{(1 + e^{-x})^2} \\\\\n",
            "&= \\frac{-e^{-x}}{1 + e^{-x}} \\cdot \\frac{1}{1 + e^{-x}} \\\\\n",
            "&= \\sigma(x) (1 - \\sigma(x)).\n",
            "\\end{aligned}\n",
            "```\n",
            "\n",
            "The derivative of the sigmoid function is important in the context of neural networks because it is used to calculate the gradient during backpropagation. The gradient is used to update the weights of the neural network during training, and the derivative of the sigmoid function helps to determine how the weights should be adjusted.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ingest PDF files\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# # split the documents into chunks\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=1000,\n",
        "#     chunk_overlap=50,\n",
        "#     separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
        "# )\n",
        "# doc_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# # Add chunk number to metadata\n",
        "# for idx, split in enumerate(doc_splits):\n",
        "#     split.metadata[\"chunk\"] = idx\n",
        "\n",
        "# print(f\"# of documents = {len(doc_splits)}\")"
      ],
      "metadata": {
        "id": "58X14Y7E6hpV",
        "outputId": "802cdd48-10a0-4ef2-e06f-9c419cefb4b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'documents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-331f22424142>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdoc_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Add chunk number to metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RFMkdr-Sie-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced: Modifying the default langchain prompt\n",
        "\n",
        "In all of the previous examples we used the default prompt that comes with Langchain.\n",
        "\n",
        "We can inspect our chain object to discover the wording of the prompt template being used.\n",
        "\n",
        "We may find that this is not suitable for our purposes, and we may wish to customise the prompt, for example to present our results in a different format, or to specify additional constraints."
      ],
      "metadata": {
        "id": "2xYMUCu_iiag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
        ")\n",
        "\n",
        "print(qa.combine_documents_chain.llm_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z4RGovgismU",
        "outputId": "aac74bf5-a519-4c87-b234-c98b343df8ff"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "{context}\n",
            "\n",
            "Question: {question}\n",
            "Helpful Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's modify the prompt to return an answer in a single word (useful for yes/no questions). We will constrain the LLM to say 'I don't know' if it cannot answer.\n",
        "\n",
        "We create a new prompt_template and pass this in using the template argument."
      ],
      "metadata": {
        "id": "Oq_sKCjwi2zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"SYSTEM: You are an intelligent research assistant helping the users with their research paper questions.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
        "\n",
        "Do not try to make up an answer:\n",
        " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
        " - If the context is empty, just say \"I do not know the answer to that.\"\n",
        "\n",
        "=============\n",
        "{context}\n",
        "=============\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "WVEBD5kviv99"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also customize the retriever"
      ],
      "metadata": {
        "id": "13V3elQKjThm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title No. of Results { display-mode: \"form\" }\n",
        "NUMBER_OF_RESULTS = \"3\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "LdzJI_QxEuKI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create chain to answer questions\n",
        "\n",
        "# Expose index to the retriever\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\n",
        "        \"k\": int(NUMBER_OF_RESULTS)\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "SEk2IWCFjUyU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    verbose=True,\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": prompt,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "67P3zYaQjhPe"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa.combine_documents_chain.llm_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCXnI6Z4jtzE",
        "outputId": "4f5fdc58-3a2d-4e00-fe9f-e5d040dab27b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: You are an intelligent research assistant helping the users with their research paper questions.\n",
            "\n",
            "Question: {question}\n",
            "\n",
            "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
            "\n",
            "Do not try to make up an answer:\n",
            " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
            " - If the context is empty, just say \"I do not know the answer to that.\"\n",
            "\n",
            "=============\n",
            "{context}\n",
            "=============\n",
            "\n",
            "Question: {question}\n",
            "Helpful Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable for troubleshooting\n",
        "qa.combine_documents_chain.verbose = True\n",
        "qa.combine_documents_chain.llm_chain.verbose = True\n",
        "qa.combine_documents_chain.llm_chain.llm.verbose = True\n",
        "\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def formatter(result):\n",
        "    print(f\"Query: {result['query']}\")\n",
        "    print(\".\" * 80)\n",
        "    print(f\"Response: {wrap(result['result'])}\")\n",
        "    print(\".\" * 80)\n",
        "    if \"source_documents\" in result.keys():\n",
        "        for idx, ref in enumerate(result[\"source_documents\"]):\n",
        "            print(\"-\" * 80)\n",
        "            print(f\"REFERENCE #{idx}\")\n",
        "            print(\"-\" * 80)\n",
        "            if \"score\" in ref.metadata:\n",
        "                print(f\"Matching Score: {ref.metadata['score']}\")\n",
        "            if \"source\" in ref.metadata:\n",
        "                print(f\"Document Source: {ref.metadata['source']}\")\n",
        "            if \"document_name\" in ref.metadata:\n",
        "                print(f\"Document Name: {ref.metadata['document_name']}\")\n",
        "            print(\".\" * 80)\n",
        "            print(f\"Content: \\n{wrap(ref.page_content)}\")\n",
        "    print(\".\" * 80)\n",
        "\n",
        "\n",
        "def wrap(s):\n",
        "    return \"\\n\".join(textwrap.wrap(s, width=120, break_long_words=False))\n",
        "\n",
        "\n",
        "def ask(query, qa=qa, k=NUMBER_OF_RESULTS, search_distance=0.5):\n",
        "    # qa.retriever.search_kwargs[\"search_distance\"] = SEARCH_DISTANCE_THRESHOLD\n",
        "    # qa.retriever.search_kwargs[\"k\"] = NUMBER_OF_RESULTS\n",
        "    result = qa({\"query\": query})\n",
        "    return formatter(result)"
      ],
      "metadata": {
        "id": "J0YbNsIukLT9"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask(query=search_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z18z8B49kdQj",
        "outputId": "68ac4f0c-0b01-4cda-d463-3124d3cddbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent research assistant helping the users with their research paper questions.\n",
            "\n",
            "Question: What should be considered when taking derivatives of ReLU?\n",
            "\n",
            "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
            "\n",
            "Do not try to make up an answer:\n",
            " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
            " - If the context is empty, just say \"I do not know the answer to that.\"\n",
            "\n",
            "=============\n",
            "identity function often helps speed up convergence, since it resembles a linear model, as long as\n",
            "the values are close to zero. Another challenge that needs to be overcome is vanishing derivatives,\n",
            "which is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not\n",
            "bounded on positive values, while also being comparatively cheap to compute, because linear\n",
            "computations tend to be very well optimized in modern computing. Altogether, these advan-\n",
            "tages have resulted in ReLU (and variants thereof) becoming the most widely used activation\n",
            "function currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was\n",
            "introduced. When taking derivatives of ReLU one needs to account for the non-diÔ¨Äerentiability\n",
            "at 0, but in numerical practice this is easily overcome.\n",
            "With the help of Neural Networks we want to solve a task, cf.\n",
            "[15, Section 5.1].\n",
            "Let the\n",
            "performance of the algorithm for the given task be measured by the loss function L, which\n",
            "\n",
            "‚áí\n",
            "y[‚Ñì] ‚àíy[‚Ñì‚àí1]\n",
            "œÑ [‚Ñì]\n",
            "= œÉ(W [‚Ñì‚àí1]y[‚Ñì‚àí1] + b[‚Ñì‚àí1]).\n",
            "4\n",
            "RESNET\n",
            "38\n",
            "Here, we consider the same activation function œÉ for all layers. Now, the left hand side of the\n",
            "equation can be interpreted as a Ô¨Ånite diÔ¨Äerence representation of a time derivative, where œÑ [‚Ñì] is\n",
            "the time step size and y[‚Ñì], y[‚Ñì‚àí1] are the values attained at two neighboring points in time. This\n",
            "relation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is\n",
            "also possible to learn the time step size œÑ [‚Ñì] as an additional variable, [2].\n",
            "Let us now introduce the diÔ¨Äerent ResNet versions from the original papers, [17, 19].\n",
            "4.1.\n",
            "DiÔ¨Äerent ResNet Versions\n",
            "In contrast to the simpliÔ¨Åed ResNet layer version (9) that we introduced, original ResNet ar-\n",
            "chitectures [17] consist of residual blocks, cf. Figure 28. Here, diÔ¨Äerent layers are grouped\n",
            "together into one residual block and then residual blocks are stacked to form a ResNet.\n",
            "y[‚Ñì‚àí1]\n",
            "Weights\n",
            "BN\n",
            "ReLU\n",
            "Weights\n",
            "BN\n",
            "+\n",
            "ReLU\n",
            "y[‚Ñì]\n",
            "\n",
            "‚àÇL\n",
            "‚àÇW [‚Ñì] = ‚àÇL\n",
            "‚àÇy[L] ¬∑\n",
            "‚Ñì+2\n",
            "Y\n",
            "j=L\n",
            "‚àÇy[j]\n",
            "‚àÇy[j‚àí1] ¬∑ ‚àÇy[‚Ñì+1]\n",
            "‚àÇW [‚Ñì] .\n",
            "In the case that we consider a very deep network, i.e. large L, the product in the derivative\n",
            "can be problematic, [5, 14], especially if we take derivatives with respect to variables from early\n",
            "layers. Two cases may occur:\n",
            "1. If\n",
            "‚àÇy[j]\n",
            "‚àÇy[j‚àí1] < 1 for all j, the product, and hence the whole derivative, tends to zero for\n",
            "growing L. This problem is referred to as vanishing gradient.\n",
            "2. On the other hand, if\n",
            "‚àÇy[j]\n",
            "‚àÇy[j‚àí1] > 1 for all j, the product, and hence the whole derivative,\n",
            "tends to inÔ¨Ånity for growing L. This problem is referred to as exploding gradient.\n",
            "Residual Networks (ResNets) have been developed in [17, 19] with the intention to solve the\n",
            "vanishing gradient problem. Employing the same notation as in FNNs, simpliÔ¨Åed ResNet layers\n",
            "can be represented in the following way\n",
            "y[‚Ñì] = y[‚Ñì‚àí1] + œÉ[‚Ñì](W [‚Ñì‚àí1]y[‚Ñì‚àí1] + b[‚Ñì‚àí1])\n",
            "for ‚Ñì= 1, . . . , L,\n",
            "(9)\n",
            "=============\n",
            "\n",
            "Question: What should be considered when taking derivatives of ReLU?\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Query: What should be considered when taking derivatives of ReLU?\n",
            "................................................................................\n",
            "Response: When taking derivatives of ReLU, one needs to account for the non-differentiability at 0. However, in numerical\n",
            "practice, this is easily overcome.\n",
            "................................................................................\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #0\n",
            "--------------------------------------------------------------------------------\n",
            "................................................................................\n",
            "Content: \n",
            "identity function often helps speed up convergence, since it resembles a linear model, as long as the values are close\n",
            "to zero. Another challenge that needs to be overcome is vanishing derivatives, which is visibly present for Heaviside,\n",
            "sigmoid and hyperbolic tangent. In contrast, ReLU is not bounded on positive values, while also being comparatively\n",
            "cheap to compute, because linear computations tend to be very well optimized in modern computing. Altogether, these\n",
            "advan- tages have resulted in ReLU (and variants thereof) becoming the most widely used activation function currently.\n",
            "As a remedy for the vanishing gradient on negative values, leaky ReLU was introduced. When taking derivatives of ReLU\n",
            "one needs to account for the non-diÔ¨Äerentiability at 0, but in numerical practice this is easily overcome. With the help\n",
            "of Neural Networks we want to solve a task, cf. [15, Section 5.1]. Let the performance of the algorithm for the given\n",
            "task be measured by the loss function L, which\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #1\n",
            "--------------------------------------------------------------------------------\n",
            "................................................................................\n",
            "Content: \n",
            "‚áí y[‚Ñì] ‚àíy[‚Ñì‚àí1] œÑ [‚Ñì] = œÉ(W [‚Ñì‚àí1]y[‚Ñì‚àí1] + b[‚Ñì‚àí1]). 4 RESNET 38 Here, we consider the same activation function œÉ for all\n",
            "layers. Now, the left hand side of the equation can be interpreted as a Ô¨Ånite diÔ¨Äerence representation of a time\n",
            "derivative, where œÑ [‚Ñì] is the time step size and y[‚Ñì], y[‚Ñì‚àí1] are the values attained at two neighboring points in\n",
            "time. This relation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is also possible to\n",
            "learn the time step size œÑ [‚Ñì] as an additional variable, [2]. Let us now introduce the diÔ¨Äerent ResNet versions from\n",
            "the original papers, [17, 19]. 4.1. DiÔ¨Äerent ResNet Versions In contrast to the simpliÔ¨Åed ResNet layer version (9) that\n",
            "we introduced, original ResNet ar- chitectures [17] consist of residual blocks, cf. Figure 28. Here, diÔ¨Äerent layers are\n",
            "grouped together into one residual block and then residual blocks are stacked to form a ResNet. y[‚Ñì‚àí1] Weights BN ReLU\n",
            "Weights BN + ReLU y[‚Ñì]\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #2\n",
            "--------------------------------------------------------------------------------\n",
            "................................................................................\n",
            "Content: \n",
            "‚àÇL ‚àÇW [‚Ñì] = ‚àÇL ‚àÇy[L] ¬∑ ‚Ñì+2 Y j=L ‚àÇy[j] ‚àÇy[j‚àí1] ¬∑ ‚àÇy[‚Ñì+1] ‚àÇW [‚Ñì] . In the case that we consider a very deep network, i.e.\n",
            "large L, the product in the derivative can be problematic, [5, 14], especially if we take derivatives with respect to\n",
            "variables from early layers. Two cases may occur: 1. If ‚àÇy[j] ‚àÇy[j‚àí1] < 1 for all j, the product, and hence the whole\n",
            "derivative, tends to zero for growing L. This problem is referred to as vanishing gradient. 2. On the other hand, if\n",
            "‚àÇy[j] ‚àÇy[j‚àí1] > 1 for all j, the product, and hence the whole derivative, tends to inÔ¨Ånity for growing L. This problem\n",
            "is referred to as exploding gradient. Residual Networks (ResNets) have been developed in [17, 19] with the intention to\n",
            "solve the vanishing gradient problem. Employing the same notation as in FNNs, simpliÔ¨Åed ResNet layers can be represented\n",
            "in the following way y[‚Ñì] = y[‚Ñì‚àí1] + œÉ[‚Ñì](W [‚Ñì‚àí1]y[‚Ñì‚àí1] + b[‚Ñì‚àí1]) for ‚Ñì= 1, . . . , L, (9)\n",
            "................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2E-IHEE5kiEp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}